{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow. Word Generation with LSTM (new model)\n",
    "\n",
    "Word generation in NLP involves creating meaningful text. Google TensorFlow, an open-source machine learning framework, offers tools to build and train models for this. This notebook covers the basics, capabilities, applications, and steps to develop word generation models using TensorFlow.\n",
    "\n",
    "![](./assets/cover.jpg)\n",
    "\n",
    "Blog Post: [TensorFlow. Word Generation with LSTM](https://vitalyzhukov.com/en/tensorflow-word-generation-with-lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "To work with TensorFlow models, you will need two libraries open-source Python libraries: NumPy (Numerical Python) and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset  \n",
    "\n",
    "To train the model, we must supply both input data and the target output. The code below creates these data sets.\n",
    "\n",
    "1. Download the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dictionary\n",
    "path_to_dict = tf.keras.utils.get_file(\n",
    "    \"popular.txt\", \"https://raw.githubusercontent.com/dolph/dictionary/master/popular.txt\"\n",
    ")\n",
    "\n",
    "# Open the dictionary\n",
    "dict = open(path_to_dict, \"rb\").read().decode(encoding=\"UTF-8\")\n",
    "\n",
    "# The dict contains one word per line. Split the text to get list of words\n",
    "words = dict.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenize the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new instanse of tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "\n",
    "# Create tokens for each character in the words list\n",
    "tokenizer.fit_on_texts(words)\n",
    "\n",
    "# Add meta-token representing end of word\n",
    "tokenizer.word_index[\"<END>\"] = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Max length of words in the vocabulary\n",
    "max_length = max(map(len, words)) + 1\n",
    "\n",
    "# Vocabulary in tensor format\n",
    "encoded_words = tokenizer.texts_to_sequences(words)\n",
    "\n",
    "meta_token = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "# Dataset to train\n",
    "input_data = []\n",
    "\n",
    "# Corresponding dataset to test\n",
    "output_data = []\n",
    "\n",
    "for seq in encoded_words:\n",
    "    # Append the end of word token\n",
    "    alt_seq = seq + [meta_token]\n",
    "    for i in range(1, len(alt_seq)):\n",
    "        input_data.append(alt_seq[:i + 1])\n",
    "\n",
    "# Make input data sequences to the same length\n",
    "input_data = np.array(tf.keras.utils.pad_sequences(input_data, maxlen=max_length, padding='pre'))\n",
    "\n",
    "input_data, validate_data = input_data[:,:-1], input_data[:,-1]\n",
    "validate_data = tf.keras.utils.to_categorical(validate_data, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a new sequential model with five layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(50,), name=\"input\"))\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(128))\n",
    "model.add(tf.keras.layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.Accuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Now, we can train the model by specifying the number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_data, validate_data, batch_size=512, epochs=10)#, callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for creating a bar chart of the prediction distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Matplotlib for Visualization purpose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_probability_bar(prefix):\n",
    "    meta_token = tokenizer.word_index[\"<END>\"]  \n",
    "    bar_chars = []\n",
    "    bar_vals = []\n",
    "\n",
    "    encoded = tokenizer.texts_to_sequences([prefix])\n",
    "    encoded = tf.keras.utils.pad_sequences(encoded, maxlen=50, padding=\"pre\")\n",
    "    predicted_characters = np.asarray(model.predict(encoded, verbose=0, batch_size=1)[0]).astype('float64')\n",
    "\n",
    "    for i in tokenizer.word_index.items():\n",
    "        key = i[0]\n",
    "        val = round(predicted_characters[i[1]], 2)\n",
    "        if meta_token == key:\n",
    "            print(val)\n",
    "        if val > 0.0: # ignore predictions with low probability\n",
    "            bar_chars.append(key.upper())\n",
    "            bar_vals.append(val)\n",
    "\n",
    "    plt.bar(bar_chars, bar_vals)\n",
    "    plt.xlabel(\"Character\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"The next character after \\\"\" + prefix + \"\\\" and its probability\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display probabilities for the input \"mic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_probability_bar(\"mic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict_next_character function returns a randomly selected character from the set of top crazy_index items, sorted by probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_character(prefix, crazy_index:int):\n",
    "    \"\"\"Predict next characters\n",
    "\n",
    "    :param prefix: Existing part of the word\n",
    "    :param crazy_index: The number of predicted characters is used to choose one.\n",
    "    :return: Predicted character\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.texts_to_sequences([prefix])\n",
    "    encoded = tf.keras.utils.pad_sequences(encoded, maxlen=50, padding=\"pre\")\n",
    "    predicted_characters = np.asarray(model.predict(encoded, verbose=0, batch_size=1)[0]).astype('float64')\n",
    "    \n",
    "    if crazy_index is None or crazy_index == 0:\n",
    "        return np.argmax(predicted_characters)\n",
    "    else:\n",
    "        if crazy_index > len(predicted_characters) : crazy_index = len(predicted_characters)\n",
    "        \n",
    "        # getting top {crazy_index} possible characters\n",
    "        candidate_args = np.argsort(predicted_characters, axis=0)[-crazy_index:]\n",
    "        probas = np.take(predicted_characters, candidate_args)\n",
    "        \n",
    "        # randomly get one the top possible characters\n",
    "        probas = np.random.multinomial(1, np.exp(np.arctan(probas))/np.sum(np.exp(np.arctan(probas))),1)\n",
    "        \n",
    "        return candidate_args[np.argmax(probas)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the entire word, we'll create a generate_words function that will call predict_next_character repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(prefix, no_words:int, crazy_index:int):\n",
    "    \"\"\"Generate words\n",
    "\n",
    "    :param prefix: Existing part of the word\n",
    "    :param no_words: Number of words to generate\n",
    "    :param crazy_index: The number of predicted characters is used to choose one.\n",
    "    :return: List of generated words\n",
    "    \"\"\"\n",
    "    max_text_lenght = 20\n",
    "    meta_token = tokenizer.word_index[\"<END>\"]  \n",
    "    words = []\n",
    "    \n",
    "    for _ in range(no_words):\n",
    "        word_prefix = prefix\n",
    "        for _ in range(max_text_lenght):\n",
    "            predicted_character = predict_next_character(word_prefix, crazy_index)\n",
    "            \n",
    "            # stop prediction if the next character is the meta token presenting end of word\n",
    "            if predicted_character == meta_token:\n",
    "                break\n",
    "            \n",
    "            # convert tensor to character\n",
    "            predicted_char = tokenizer.sequences_to_texts([[predicted_character]])\n",
    "            \n",
    "            # append the character to the result word\n",
    "            word_prefix = word_prefix + predicted_char[0]\n",
    "                \n",
    "        words.append(word_prefix)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_words(\"\", 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for Future Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"new_model_word_generation.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
